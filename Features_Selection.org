-*- Mode: org; mode: auto-fill; fill-column: 76 -*-

#+SEQ_TODO: TODO(t) STARTED(s!) SOMEDAY(S!) WAIT(w@/!) DELEGATE(e@/!) | DONE(d!/!)  CANCELED(c@)
#+STARTUP: overview
#+STARTUP: lognotestate
#+TAGS: noexport(n) export(e)
#+PROPERTY: Effort_ALL 0 0:10 0:20 0:30 1:00 2:00 4:00 6:00 8:00

#+TITLE:     Features_Selection
#+AUTHOR:    Leandro Fernandes
#+EMAIL:     leandro_h_fernandes@cargill.com
#+DATE:      <2015-12-09 Wed>

#+LANGUAGE:  en
#+TEXT:      GTD Agenda
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t TeX:t LaTeX:nil skip:t d:nil tags:not-in-toc
#+INFOJS_OPT: view:overview toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+LINK_UP:
#+LINK_HOME:
#+PROPERTY: Effort_ALL 0:05 0:15 0:30 0:45 1:00 1:30 2:00 3:00 4:00 5:00
#+TAGS: DATA(d) MODELLING(m) FORECASTING(f) WRITTING(w) REFACTORING(r)
#+COLUMNS: %40ITEM(Task) %TODO %17Effort(Estimated Effort){:} %CLOCKSUM %TAGS

# Local Variables:
# org-export-html-style: "   <style type=\"text/css\">
#    a:link, a:visited {font-style: italic; text-decoration: none; color: black; }
#    a:active {font-style: italic; texit-decoration: none; color: blue; } </style>
#   </style>"
# End:


#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export

* Introducao
    
* Analysis
  
  #+begin_src R
    source('libs/features_selection.R')
    source('libs/data_simulator.R')

    data.sim    <- DataSimulator(200)
    data.formula <- data.sim$GetVarsFormula()
    reg.formula <- formula(paste0(data.formula," + I(x1^2) + x2:x3"))

    db       <- data.sim$GetData()
    train.db <- db[seq(1,150,by=1),]
    test.db  <- db[seq(151,200,by=1),]
    bayes.error = data.sim$GetSigma()


    nvmax <- 15
    reg.exp <- RegsubsetExplorer(train.db,test.db,reg.formula,nvmax,
                                 nbest=1,really.big=FALSE,force.in=NULL)

    reg.exp$GetRegsubsetDashBoard(bayes.error)

    param <- list("objective" = "reg:linear",
                  "eta" = 0.1,
                  ##"min_child_weight" = 5,
                  "subsample" = 0.80,
                  "colsample_bytree" = 0.80,
                  "scale_pos_weight" = 1.00,
                  "silent" = 1,
                  ##"booster" = "gbtree",
                  "max_depth" = 7,
                  "seed" = 19)

    number.of.models <- 15
    xgb.exp <- XGBoostExplorer(train.db, test.db, "y", number.of.models,
                               param)

    xgb.exp$PlotRelativeImportance()
    xgb.exp$GetXGBoostDashBoard(bayes.error)
    xgb.exp$GetTestErrorGridDashBoard(bayes.error)


    rf.exp <-

        RandomForestExplorer(train.db, test.db, data.formula, number.of.models)

    rf.exp$GetRandomForestDashBoard(bayes.error)
  #+end_src

* Data
** Setup

#+name: setup_block
#+begin_src R :tangle RCode/setup_block.R :exports none :session
  #* ****************************************************************
  #  Programer[s]: Leandro Fernandes
  #  Company/Institution: Cargill
  #  email: leandro_h_fernandes@cargill.com
  #  Program: Setup
  #  Date: January 4, 2015
  #
  #  This work is resulted from Author's hard work. The author
  #  believes that sharing code, skills and knowledge is fantastic.
  #  But please remember to cite the author and give him Its
  #  credit. (Don't be a jerk and steal his credit and ideas)
  #* ****************************************************************

  cat(" === setup_block === \n")
  require(RODBC)   ## TODO: Verificar se estou usando este package
  require(RJDBC)
  require(lubridate)
  require(R.utils) ## sourceDirectory
  ## require(plyr)
  require(scales)  #library(colorspace)
  require(leaps)   ## regsubsets automatic model selection
  require(sqldf)
  require(tcltk)   # better than file.choose , with more options
  require(reshape2)
  require(xlsx)
  require(zoo)
  require(ggplot2)
  require(session)
  require(car)     #vif
  require(psych)
  require(RUnit)

  proj.dir <- getwd()
  cat("Proj dir: ", proj.dir,"\n")

  # Loading fucntions
  cat("Loading SourceDir\n")
  source(file.path(proj.dir,'Rcode/setup/SourceDir.R'))
  cat("Loading functions\n")
  SourceDir(file.path(proj.dir,'Rcode/computations'))
  SourceDir(file.path(proj.dir,'Rcode/data_mugging'))
  SourceDir(file.path(proj.dir,'Rcode/data_design'))
  SourceDir(file.path(proj.dir,'Rcode/exploratory'))
  SourceDir(file.path(proj.dir,'Rcode/model'))
#+end_src

* Preprocessing
* Analises
* Forecast e Monitoramento
* Conclusao
* Appendix
** R functions

   [[file:Rcode/main_block.R]]

#+name: main_block
#+begin_src R :tangle Rcode/main_block.R :session :exports none


  source("Rcode/setup_block.R")
  source("Rcode/load_new_data_block.R")
  source("Rcode/data_mugging_block.R")
  source("Rcode/data_design_block.R")
  source("Rcode/analyses_block.R")
  # Interactive graph gen error while running inside emacs
  source("Rcode/model_block.R")
  source("Rcode/forecast_block.R")

#+end_src
*** Data Design

    [[file:Rcode/data_design.R]]

#+begin_src R :tangle Rcode/data_design.R :exports none

  LoadNewData <- function()
  {
     return(list(NULL))
  }

  BuildDataTarget <- function( )
  {

    return(data.target)
  }

  BuildNewData <- function()
  {

    return(new.data)
  }
#+end_src

*** Exploratory functions

    [[file:Rcode/exploratory_data.R]]

#+begin_src R :tangle Rcode/exploratory_data.R :exports none

# * **********************************************************************
#   Programer[s]: Leandro Fernandes
#   Company/Institution: Cargill
#   email: leandro_h_fernandes@cargill.com
#   Date: Dec 2013
# * **********************************************************************

#+begin_src R

PrintSummaryReport <- function(res.table){

  # relative error: (Yi - Model)/Yi
  res     <- res.table[ res.table$res_type == "train", "res"]
  res.rel <- res.table[ res.table$res_type == "train", "res_rel"]
  cat("summary res in train db: \n")
  print(summary(res))
  cat("summary relative res in train db: \n")
  print(summary(res.rel))

  # counting the number between +/- 0.05
  cat("counting the number between +/- 0.05\n")
  print(length(res.rel[res.rel > -0.05 & res.rel < 0.05]))
  cat("Percentage between +/- 0.05\n")
  print(length(res.rel[res.rel > -0.05 & res.rel < 0.05])/length(res.rel))

  res.thr <- -0.05
  res.type.pos <- which( names(res.table) == "res_type")
  cat("Residual less than ",  res.thr , "\n")
  print(res.table[ res.table$res_type == "train" &  res.table$res_rel < res.thr, -res.type.pos ])

  res.thr <- 0.05
  cat("Residual greater than ", res.thr ,"\n")
  print(res.table[ res.table$res_type == "train" &  res.table$res_rel > res.thr, -res.type.pos])

  res.thr <- -0.10
  cat("relative residual: outliers <", res.thr, "\n")
  print(res.table[ res.table$res_type == "train" &  res.table$res_rel < res.thr, -res.type.pos])
}

boxplotExplorer <- function(y,x,ynew=NULL,xnew=NULL,
                            thr.min=NULL,thr.max=NULL,
                            main=NULL,ylab=NULL,xlab=NULL,fpath=NULL){
  # plot boxplot(y ~ x, main,xlab)
  # add newpoints given by xnew and ynew in the boxplot output
  # save copy of the graph in the file
  # x,y aredata
  # xnew,ynew are discrete points to show with the each boxplot
  # th.min, thr.mas put horizontal lines in the graph. The lines position
  # is define by thr.min n thr.max

  tryCatch (
  {
    nx  <- length(xnew)
    ny  <- length(ynew)

    boxplot(y ~ x, main=main,ylab=ylab,xlab=xlab)

    #cat("ny: ",ny)
    #cat("nx: ",nx)
    if(nx > 0 & nx <= ny){
      for (k in seq(1,nx,by=1)){
        if(!is.na(ynew[k])){
            #cat("k: ",k)
            abline(ynew[k],0,col=k)
            points(xnew[k],ynew[k],pch=16,col=k)
        }
      }
    }

    if(!is.null (thr.max) & !is.null (thr.max) ){
      #Plot zero line when zero is between min n max thr
      if(thr.max*thr.min < 0)abline(0,0,col="red",lty = 2)
      abline(thr.max,0,col="blue",lty = 2)
      abline(thr.min,0,col="blue",lty = 2)
    }

    #cat("full.path: ",fpath)
    if(!is.null (fpath)){

      dev.copy(png,file.path(fpath))
      dev.off()
    }
  },
  interrupt = function(ex){cat("An interrupt was detected.\n"); print(ex);},
  error = function(ex)     {cat("An error was detected.\n")    ; print(ex);}
  )
}

#+end_src

*** Data mugging functions

    [[file:Rcode/clean_data.R]]

#+begin_src R :tangle Rcode/clean_data.R :exports none

# * **********************************************************************
#   Programer[s]: Leandro Fernandes
#   Company/Institution: Cargill
#   email: leandro_h_fernandes@cargill.com
#   Date: Dec 2013
# * **********************************************************************

show_fields_with_na <- function(df){
  # Usado para debug e conferencias
  # try: show_fields_with_na(routes_tabular_monthly)

  nc <- ncol(df)
  nr <- nrow(df)

  count_acc <- 0
  for (i in 2:nc){

    number_of_na <- sum(is.na(df[,i]))
    #if(number_of_na > 0){
    cstr <- class(df[1,i])
    print(paste("field name: ",   names(df)[i],  "number of NA:", number_of_na ))
    #}
  }
}

#+end_src

#+begin_src R :tangle Rcode/clean_data.R :exports none


# -------------------------------------- [ inteprolation ]
# XXX This code is data dependent. This is not good. What happen if I change routes ID? I have to re-write
# this code after.


## XXXX Write a function to avoid redudant code like: (see interpolation_matopi code wher I interpolate 1 route)
## I am getting route, getting km, do math, rewrite new ropute value
## I am doing this for everu roiute I have to interpolate

interpolation_br <- function(routes_table){

  routes_tabular_cleared <- interpolation_ba(routes_table)

  routes_tabular_cleared <- interpolation_go(routes_table)

}

interpolation_ba <- function(routes_table){

  # id(155) = id(153) + 21
  # id(156) = id(153) + 24

  #   COAC-ATU e ESTRONDO - ATU vou usar LEM-ATU
  #
  #   LEM-ATU 2007-2008 = tku(BRS-ATU)* km(lem)
  #   CORREL(LEM-ATU,BRSR-ATU) = 0.50

  km_coaceral_atu <- 1110.00
  km_estrondo_atu <- 1087.00
  km_lem_atu <- 955.00
  km_brs_atu <- 890.00

  coaceral_atu.tku <- routes_table$"155"
  # coaceral_atu.tku <- get_freight_table(key_type="alias",key_value="BA_CCR_ATU",2007,db_file)$avg_tku

  estrondo_atu.tku <- routes_table$"156"
  lem_atu.tku <- routes_table$"153"
  brs_atu.tku  <- routes_table$"165"

  # lem_atu with na first week
  #lem_atu.tku[1] <- lem_atu.tku[2]

  lem_atu.pos <- is.na(lem_atu.tku)
  lem_atu.tku[lem_atu.pos] <- brs_atu.tku[lem_atu.pos]

  coaceral_nas.pos <- is.na(coaceral_atu.tku)
  coaceral_atu.tku[coaceral_nas.pos] <- (lem_atu.tku[coaceral_nas.pos]*km_lem_atu + 21.00)/km_coaceral_atu
  estrondo_nas.pos <- is.na(estrondo_atu.tku)
  estrondo_atu.tku[estrondo_nas.pos] <- (lem_atu.tku[estrondo_nas.pos]*km_lem_atu + 24.00)/km_estrondo_atu

  # re-writing everyting

  routes_table$"155" <- coaceral_atu.tku
  routes_table$"156" <- estrondo_atu.tku
  routes_table$"153" <- lem_atu.tku

  return(routes_table)
}

interpolation_go <- function(routes_table){

  # id(50) RVD-SSM = id(51) RVD-ULA - 13
  # id(52) RVD-PGA = id(49) RVD-GJA - 10

  # Maior correl foi com ULA: 0.71 e CV: 19%
  # Com GJA (a outra opcao) : corr = 0.48 e CV = 13%

  # "field name:  52 number of NA: 24" *RVD-PGA*  = RVD-GJA - 10

  km_rvd_ssm <- 211.00
  km_rvd_ula <- 324.00
  km_rvd_pga <- 1361.00
  km_rvd_gja <- 1050.00

  rvd_ssm.tku <- routes_table$"50"
  rvd_ula.tku <- routes_table$"51"

  rvd_pga.tku <- routes_table$"52"
  rvd_gja.tku <- routes_table$"49"

  rvd_ssm.pos <- is.na(rvd_ssm.tku)
  rvd_pga.pos <- is.na(rvd_pga.tku)

  rvd_ssm.tku[rvd_ssm.pos] <- (rvd_ula.tku[rvd_ssm.pos]*km_rvd_ula - 13.00)/km_rvd_ssm
  rvd_pga.tku[rvd_pga.pos] <- (rvd_gja.tku[rvd_pga.pos]*km_rvd_gja - 10.00)/km_rvd_pga

  # re-writing evryting
  routes_table$"50" <- rvd_ssm.tku
  routes_table$"52" <- rvd_pga.tku

  return(routes_table)
}

interpolation_matopi <- function(routes_table){

  # id(187) = id(173) BALSA-PFC - 14

  # "field name:  187 number of NA: 24" AZM_ALVORADA-PFC =
  #   BALSA-PFC - 14

  # CORREl(BALASA,ALVORADA) = 0.84

  km_arm_alvorada_pfc <- 770.00
  km_arm_balsa_pfc <- 820.00

  arm_alvorada_pfc.tku <- routes_table$"187"
  arm_balsa_pfc.tku <- routes_table$"173"

  arm_alvorada_pfc.pos <- is.na(arm_alvorada_pfc.tku)

  arm_alvorada_pfc.tku[arm_alvorada_pfc.pos] <- (arm_balsa_pfc.tku[arm_alvorada_pfc.pos]*km_arm_balsa_pfc - 14.00)/km_arm_alvorada_pfc

  # re-writing evryting
  routes_table$"187" <- arm_alvorada_pfc.tku

  return(routes_table)
}

interpolation_sp <- function(routes_table){

  # "field name:  242 number of NA: 24" BIRIGUI-GJA = GUAIRA + 1
  # "field name:  244 number of NA: 2"  GUAIRA-GJA *NAO achei este MISSING*
  # "field name:  248 number of NA: 24" TAQ-GJA: GUAIRA + 18

  # CORREL(Birigui,Giuaira) = 0.93
  # CORREL(TAquarituba,Giuaira) = 0.0.73

  km_birigui_gja <- 625.00
  km_taq_gja <- 440.00
  km_guaira_gja <- 607.00

  birigui_gja.tku <- routes_table$"242"
  taq_gja.tku <- routes_table$"248"
  guaira_gja.tku <- routes_table$"244"

  # linear interpolation (See end of 2008)
  guaira_gja.tku <- na.approx(guaira_gja.tku)

  birigui_gja.pos <- is.na(birigui_gja.tku)
  taq_gja.pos <- is.na(taq_gja.tku)

  birigui_gja.tku[birigui_gja.pos] <- (guaira_gja.tku[birigui_gja.pos]*km_guaira_gja + 1.00)/km_birigui_gja
  taq_gja.tku[taq_gja.pos] <- (guaira_gja.tku[taq_gja.pos]*km_guaira_gja + 18.00)/km_taq_gja

  # re-writing evryting
  routes_table$"242" <- birigui_gja.tku
  routes_table$"248" <- taq_gja.tku
  routes_table$"244" <- guaira_gja.tku

  return(routes_table)
}

interpolation_ms <- function(routes_table){
  #   1. "field name:  75 number of NA: 34" Campo Grande->GJA
  #   2. "field name:  76 number of NA: 34" Campo Grande->TLG
  #   3. "field name:  77 number of NA: 34" CHPS-GJA
  #   4. "field name:  78 number of NA: 34" CHPS-TLG
  #
  #   *CORR(CHP-GJA,TLG-GJA) = 0.81*
  #     CORR(CHP-GJA,DRS-GJA) = 0.67

  #   GJA contra TLG spread do valor medio nos picos de fretes
  #   mais antigo no historico

  #   *Campo grande vou usar TLG -GJA pq o cam passa por TLG.*

  #   CHP-GJA = TLG-GJA + 17
  #   CHP-TLG = CHP-GJA + 88
  #
  #   CMP-GJA = TLG-GJA + 10
  #   CMP-TLg = CMP-GJA + 90

  km_campo_grande_gja <- 1063.00
  km_campo_grande_tlg <- 338.00
  km_chps_gja <- 987.00
  km_chps_tlg <- 381.00
  km_tlg_gja <- 714.00

  campo_grande_gja.tku <- routes_table$"75"
  campo_grande_tlg.tku  <- routes_table$"76"
  chps_gja.tku <- routes_table$"77"
  chps_tlg.tku <- routes_table$"78"
  drs_tlg.tku <- routes_table$"81"

  tlg_gja.tku <- routes_table$"96"

  campo_grande_gja.pos <- is.na(campo_grande_gja.tku)
  campo_grande_tlg.pos <- is.na(campo_grande_tlg.tku)

  chps_gja.pos <- is.na(chps_gja.tku)
  chps_tlg.pos <- is.na(chps_tlg.tku)

  campo_grande_gja.tku[campo_grande_gja.pos] <- (tlg_gja.tku[campo_grande_gja.pos]*0.80)
  campo_grande_tlg.tku[campo_grande_tlg.pos] <- (drs_tlg.tku[campo_grande_tlg.pos]*1.03)

  chps_gja.tku[chps_gja.pos] <- (tlg_gja.tku[chps_gja.pos]*0.86)
  chps_tlg.tku[chps_tlg.pos] <- (chps_gja.tku[chps_tlg.pos]*1.03)

  # re-writing evryting
  routes_table$"75" <- campo_grande_gja.tku
  routes_table$"76" <- campo_grande_tlg.tku

  routes_table$"77" <- chps_gja.tku
  routes_table$"78" <- chps_tlg.tku

  return(routes_table)
}
#+end_src

*** Computations

    [[file:Rcode/computation.R]]

**** Freight average

#+begin_src R :tangle Rcode/computation.R :exports none

# * **********************************************************************
#   Programer[s]: Leandro Fernandes
#   Company/Institution: Cargill
#   email: leandro_h_fernandes@cargill.com
#   Date: Dec 2013
# * **********************************************************************

# Compute reg avg

compute_freight_avg_compute_reg_avg <- function(routes_tabular,route_list){
  # Compute avg for: MT,PR,MS,GO,MG,MS,SP,MATOPI and RS

  reg_routes_df <- data.frame(id=numeric(),
           year_month_day=character(),
           freight=numeric(),
           tku=numeric())

  ids_list <- route_list$id

  field_names <- names(routes_tabular)

  pos_list <- NULL
  for (id in ids_list ){
      pos <- which(field_names == id)
      pos_list <- c(pos_list,pos)
  }

  tku_avg_aux <- rowMeans(routes_tabular[ ,pos_list ])
  reg_tku_avg <- data.frame(year_month=routes_tabular[ ,1], avg_tku=tku_avg_aux)

  return(reg_tku_avg)
}

compute_freight_avg_compute_ba_avg <- function(freight_data_frame,route_list){
  # Compute avg for: BA

  # <BA> 50% avg(Barreiras) + 35% avg(Aratu) + 15% avg(ilheus)
  # Vou fazer mais simples por qustoes de tempo.

  to_aratu_routes <- data.frame(reg = rep("BA",4), id = c(153,155,156,165))
  to_ilheus_routes <- data.frame(reg = rep("BA",1), id = c(164))
  to_barreiras_routes <- data.frame(reg = rep("BA",7), id = c(142,143,145,147,148,150,152))

  to_aratu_df <- compute_freight_avg_compute_reg_avg(freight_data_frame,to_aratu_routes)
  to_barreiras_df <- compute_freight_avg_compute_reg_avg(freight_data_frame,to_barreiras_routes)

  pos <- which(names(freight_data_frame)== "164", arr.ind = TRUE)
  to_ilheus_df <- freight_data_frame[,c(1,pos)]

  ba_avg_tku <- 0.5*to_barreiras_df[,2] + 0.35*to_aratu_df[,2] + 0.15*to_ilheus_df[,2]

  ba_avg_df <- data.frame(year_month=to_barreiras_df$year_month,avg_tku=ba_avg_tku)

  rm(to_aratu_df)
  rm(to_ilheus_df)
  rm(to_barreiras_df)

  return(ba_avg_df)

}

compute_freight_avg_compute_br_avg <- function(freight_data_frame,route_list){
  # Compute avg for: MT,PR,MS,GO,MG,MS,SP,MATOPI and RS

  weight_df <- data.frame(reg = c("BA", "GO", "MATOPI", "MG", "MS","MT", "PR","RS","SP"),
                          weight = c(0.04,0.10,0.04,0.05,0.05,0.30, 0.20, 0.12, 0.10))

  # start with MT
  r <- "MT"
  w <- weight_df[ weight_df$reg == r , 2]

  mt_routes_list <- subset(route_list,route_list$reg== "MT")

  mt_avg_df <- compute_freight_avg_compute_reg_avg(freight_data_frame,mt_routes_list)
  weight_avg_tku <- mt_avg_df$avg_tku*w

  for(r in c("PR","MS","GO","MG","MS","SP","MATOPI","RS")){

    #cat(paste("----------- [",r," ] ------\n"))
    w <- weight_df[ weight_df$reg == r , 2]

    # get regional routes subset
    reg_routes_list <- subset(route_list,route_list$reg== r)

    # compute regional average
    reg_avg_df <- compute_freight_avg_compute_reg_avg(freight_data_frame,reg_routes_list)

    # compute weight average
    weight_avg_tku <- weight_avg_tku + reg_avg_df$avg_tku*w
  }

  br_weight_avg <- data.frame(year_month_day=mt_avg_df$year_month,avg_tku=weight_avg_tku)

  return(br_weight_avg)
  #return(1)
}

compute_freight_avg <- function(reg,freight_data_frame,route_list){
  # Compute avg for: MT,PR,MS,GO,MG,MS,SP,MATOPI and RS

  avg <- NULL
  ## TODO Put it in csv file. It is better because qe can use excel to easy edit
  ## Add: MA: 0.04, DF: 0.05 , TO: 0.04 and PI: 0.04
  ## XXX Sum(weight_df$weight[1:9]) = 1.0
  weight_df <- data.frame(reg = c("BA", "GO", "MATOPI", "MG", "MS","MT", "PR","RS","SP"),
                          weight = c(0.04,0.10,0.04,0.05,0.05,0.30, 0.20, 0.12, 0.10))

  if(reg == "MT" | reg == "PR" | reg == "MS" | reg == "GO"
     | reg == "MG" | reg == "MS" | reg == "SP" | reg == "MATOPI" |
       reg == "RS"){

    avg <- compute_reg_avg(freight_data_frame,route_list)
  }

  if(reg == "BA"){
    avg <- compute_ba_avg(freight_data_frame,route_list)
  }

  if(reg == "BR"){

    br_avg_df <- compute_freight_avg_compute_br_avg(freight_data_frame,route_list)
  }

    # Compute BA avg
    w <- weight_df[ weight_df$reg == "BA" , 2]
    avg_list[9] <- compute_freight_avg_compute_ba_avg(freight_data_frame,route_list)
    avg_list[9] <- avg_list[9]*w

    # BR avg
    avg <- sum(avg_list)

  return(avg)
}

#+end_src

**** Freight variations

#+begin_src R :tangle Rcode/computation.R :exports none

get_variation <- function( monthly_tku){

  n <- length(monthly_tku)
  d_tku <- diff(monthly_tku)
  r <- d_tku/monthly_tku[-n]

  return (r)
}

get_annual_variation_table <- function( data_dates, data_values){

  # XXX just to be smart n quickly
  route_table <- data.frame(year_month=data_dates,data_values=data_values)

  df <- data.frame(year=as.character(),month=as.character(),tku=as.numeric(),variation=as.numeric())
  for(m in seq(1,12)){

    str_date <- route_table[ as.numeric(substr(route_table[,1],6,7)) == m ,1]
    #str_date <- data_dates # FORMAT: YYY-MM
    str_year <- substr(str_date,1,4)
    str_month <- substr(str_date,6,7)

    m_tku <- route_table[ as.numeric(substr(route_table[,1],6,7)) == m ,2]
    #data_value_m <- data_values[ as.numeric(str_month) == m ]

    d_aux_tku <- get_variation(m_tku)
    d_tku <- append(NA,d_aux_tku)
    #last_year_variation <- append(NA,get_variation(data_value_m))

    df <- rbind(df,data.frame(year=str_year, month=str_month,tku=m_tku,variation=d_tku))
    #df <- rbind(df,data.frame(year=str_year, month=str_month,tku=m_tku,variation=d_tku))
  }

  return (df)
}

get_variation_table <- function( routes_table_monthly){

  ## TODO Improve to consider missing 2009 Nov and Dec.
  ## Jan-10 doesn't make sense

  ## XXX Implement it to make it more generical like passing id_list
  ## List of key route in file var_monthly.xlsx
  id_list <- c("1","2","9","10","25","26","99","108","129")

  df <- data.frame(id=as.character(),year=as.character(),month=as.character(),tku=as.numeric(),variation=as.numeric())
  for (id in id_list){
    route_df <- routes_table_monthly[ routes_table_monthly$ID == id,c(2,4)]

    number_of_samples <- length(route_df$avg_tku)
    y <- substr(route_df$year_month,1,4)
    m <- substr(route_df$year_month,6,7)

    r <- get_tku_variation( route_df$avg_tku)

    df <- rbind(df,data.frame(id=rep(id,number_of_samples-1),year=y[-1],month=m[-1],tku=route_df$avg_tku[-1],variation=r))
  }

  return (df)
}

ComputeMonthlyVariation <- function(data.target, new.data,len.new.data){
  # data.target historical data being investigates
  # new.data the data in period being modelled (forecast period)

  # Compute br_tku and combined monthly variation

  # ---------- building br_tku monthly variation table
  # variacao mensal br_tku
  dbr.tku <- append(NA,get_variation( data.target$br_tku ))

  # Lembre-se dos missing em 2009
  dbr.tku[ data.target$year == 2010 & data.target$month == 1] <- NA

  # dealing with new year: Dec 2013 -> Jan 2014
  dec.2013 <- data.target[ data.target$date == "2013-12-15" ,"br_tku"]
  dbr.tku.aux <- new.data$br_tku[1]/dec.2013 - 1.0

  # dbr.tku.new <- append(dbr.tku.aux,get_variation( new.data$br_tku[1:len.new.data] ))
  dbr.tku.new <- append(dbr.tku.aux,get_variation( new.data$br_tku[1:len.new.data] ))
  dbr.tku.new <- append(dbr.tku.new,rep(NA,12 - len.new.data))

  # ---------- building combined monthly variation table
  dcombined <- append(NA,get_variation( data.target$combined ))

  #dcombined.monthly.variation <- data.frame(year=data.target$year,month=data.target$month,variation=dcombined.aux)

  # dealing with new year: 2013 -> 2014
  dec.2013 <- data.target[ data.target$date == "2013-12-15","combined"]
  dcomb.aux <- new.data$combined[1]/dec.2013 - 1.0

  dcomb.new <- append(dcomb.aux,get_variation( new.data$combined ))

  data.variation  <- data.frame( year		= data.target$year,
                                 month	= data.target$month,
                                 br_tku	= dbr.tku,
                                 combined	= dcombined)


  new.data.variation <- data.frame( year      = new.data$year,
                                    month	= new.data$month,
                                    br_tku	= dbr.tku.new,
                                    combined	= dcomb.new)

  return(list(data.variation,new.data.variation))
}

ComputeAnnualVariation <- function(data.target, new.data,len.new.data){
  # data.target historical data being investigates
  # new.data the data in period being modelled (forecast period)

  # Compute br_tku, harvest and combined annual variation

  # tku annual variation
  dbr.tku <- get_annual_variation_table(data.target$date,data.target$br_tku)

  # append 2014
  date.aux   <- append( data.target[ data.target$year == "2013","date" ]  , new.data$date)
  br.tku.aux <- append( data.target[ data.target$year == "2013","br_tku" ], new.data$br_tku)

  dbr.tku.new <- get_annual_variation_table( date.aux , br.tku.aux )

  # Removing NAs in 2007
  dbr.tku <- dbr.tku[ complete.cases(dbr.tku) ,]
  # Removing NAs in 2013
  dbr.tku.new <- dbr.tku.new[ dbr.tku.new$year == "2014" ,]

  # Combined
  dcomb <- get_annual_variation_table(data.target$date, data.target$combined )

  # append 2014 in br_avg 2013
  date.aux <- append(data.target[ data.target$year == "2013","date" ],new.data$date)
  comb.aux <- append(data.target[ data.target$year == "2013","combined"],new.data$combined)

  dcomb.new <- get_annual_variation_table( date.aux , comb.aux )

  dcomb <- dcomb[ complete.cases(dcomb), ]
  dcomb.new <- dcomb.new[ complete.cases(dcomb.new),]

  # harvest
  dharv <- get_annual_variation_table( data.target$date, data.target$harvest)
  # Removing Na in 2007
  dharv <- dharv[ !(dharv$year == "2007"), ]

  # append 2014 in br_avg 2013
  harv.aux <- append( data.target[ data.target$year == "2013","harvest" ], new.data$harvest)

  # get 2014
  dharv.new <- get_annual_variation_table( date.aux , harv.aux )
  # Removing 2013 with NA
  dharv.new <- dharv.new[ !(dharv.new$year == "2013"),]

  # Replacing inf, Nan values by NA
  dharv[is.infinite(dharv$variation),]		<- NA
  dharv[is.nan(dharv$variation),]		<- NA

  dharv.new[is.infinite(dharv.new$variation),]	<- NA
  dharv.new[is.nan(dharv.new$variation),]	<- NA

  data.variation  <- data.frame( year		= dbr.tku$year,
                                 month		= dbr.tku$month,
                                 br_tku		= dbr.tku$variation,
                                 harvest	= dharv$variation,
                                 combined	= dcomb$variation)


  new.data.variation <- data.frame( year	= dbr.tku.new$year,
                                 month		= dbr.tku.new$month,
                                 br_tku		= dbr.tku.new$variation,
                                 harvest	= dharv.new$variation,
                                 combined	= dcomb.new$variation)

  return(list(data.variation,new.data.variation))
}
#+end_src

*** Model

    [[file:Rcode/model.R]]

#+begin_src R :tangle Rcode/model.R :exports none :session

  BuildResidualsTable <- function(model,train.db,test.db){

      res.rel <- model$residuals/train.db$br_tku
      res.table.train <- data.frame(year = train.db$year, month=train.db$month,
                                    res = model$residuals, res_rel = res.rel,
                                    res_type = rep("train",length.out= length(res.rel)),
                                    fit = model$fit, br_tku = train.db$br_tku)

      res.table.train <- cbind(res.table.train,
                               train.db[, c("harvest","exp_soy","exp_corn","exp_sugar",
                                            "del_fert","cams")])

      pred <- predict(model, test.db, interval="pred")
      res.test <- test.db$br_tku -  pred[,1]
      res.rel.test <- res.test/test.db$br_tku

      # res.type <- append(rep("test", length.out= length(test.db[test.db$year < 2014,1])),
      #                   rep("new",  length.out= length(test.db[test.db$year == 2014,1] )))
      res.type <- rep("test", length.out= length(res.test))

      res.table.test <- data.frame(year = test.db$year, month=test.db$month,
                                    res = res.test, res_rel = res.rel.test,
                                    res_type = res.type,
                                    fit = pred[,1], br_tku = test.db$br_tku)

      res.table.test <- cbind(res.table.test,
                              test.db[, c("harvest","exp_soy","exp_corn","exp_sugar",
                                            "del_fert","cams")])
      res.table <- rbind(res.table.train,res.table.test)
      return(res.table)
  }
#+end_src
** Project Tree (Folders)
#+BEGIN_FSTREE: . :relative-links t :non-recursive nil
#+END_FSTREE
** Unit Tests							   :noexport:

   [[file:Rcode/main_tests_block.R]]

#+name: tests_block
#+begin_src R :tangle tests/main_tests_block.R :exports none :session

  # * **********************************************************************
  #   Programer[s]: Leandro Fernandes
  #   Company/Institution: Cargill
  #   email: leandro_h_fernandes@cargill.com
  #   Commentary: Unit tests
  # * **********************************************************************

  cat(" === tests_block === \n")
  library('testthat')
  require(tools) # md5sum
  test_dir('tests', reporter = 'Summary')
#+end_src
* Automate system 						   :noexport:
** Loaded Questions

   *Projetos precisam ter comeco , muio e fim alem de um objetivo claro.*

   1. Goals scope:
      1. Generic one
	 1. Qual eh o escopo? O objetivo? Nunca se esqueca disto.
	    Construir uma ferramenta para calssifcar se um email eh spam ou NAO
      2. Specific

	 Usando os dados do site S, investigar as vars Xs e construir um
         calssificador de emails (SPAM ou NAO) utilizando uma das tecnicas:
         T1, T2 or T3.

   2. Data scope: *MECE* (mutually exclude collected exhaustive)
      1. Data
	 1. Quais dados tenho confianca? E quais nao tenho tanta assim?
	 2. Os dados sao adequados para o escopo do modelo?
	 3. Tenho projecao destes dado? Sao boas estas projecoes?
      2. Ys:
	 1. Quais periodos tem maior volatilidade?
	 2. Quais periodos podemos ter inversao (As veze sobe as vezes cai)?
            Alerta onde podemos erra a direcao. (Preciso calcular as variacoes
            temporais)
      3. Xs:
	 1. Definir quais variaveis serao investigadas. Manter o FOCO
	 2. ADD alguma coisa aqui
   3. Modelo
      1. Oq nao considerei qual seria o palpite intuitivo de como ele
         afetaria minha projecao? Consigo ver esas relacoes olhando para os
         residuos e estes fatores que nao estou considerando?
   4. Res:
      1. Quais periodos os residuos apresentam bias?
      2. Qual periodo os residuos apresentam grande variacao? Posso errar pr
         pouco ou por muito.
   5. Forecast
      1. Como estao as projecoes de Xs em relacao a base historica?
      2. Como minha projecao estah em relacao a base historica? (Acima do
         ano passad abaixo. Faz sentido?)
      3. As variacoes temporais (mensai, anuais) da projecao sao compativeis
         com estas mesamas variacoes na base? Faz sebtido?
   6. Aplicacao do modelo (Impacto) *<=* (Um dos mais importnates dos items)
      1. Quais perguntas eu consigo responder com o atual modelo?
	 1. Pensar na aplicaco ao negocio
      2. Tipos de perguntas comuns para responder
	 1. Oq vai acontercer se ocorrer uma reducao de 10%X na var Y?
	 2. Pq aconteceu esta queda.
	 3. Oq irá acontecer?

** Analytical process Concepts

1. *Versionado* (SVN, GIT e tortoise)

   1. *Evolui continuamente a passos pequenos*
   2. Evita re-trabalho
   3. Registro do projeto no tempo. Mantém analise transparente.

2. *Work in pairs*

   1. Ajuda prevenir blind-spots.
   2. Acelera curvas de aprendizado
   3. *Permite construcao de buy-in qdo ooutro par eh da area cliente*

3. *Reproduzivel* Porque?

   1. Nos mantém honesto,
   2. Permite rever os passos,
   3. Permite outros rodarem o modelo e assim permite aprendizado
   4. As coisas continuam funcionado caso eu nao esteja

4. *Documentacao Interna* (Confidencial e pertence ao GTABR)

   1. Salvar a expertise adquirida.
   2. Ajuda organizar suas ideias. (Qdo vc se obriga a escrever isto de
      forac a pensar e rever suas ideais)
   3. Qdo for questionado por algo que fez muito tempo atrás, pode-se
      consultar a doc.
   4. Permite outros aproveitar a experiencia adquirida e/ou adaptar
      para o seu caso.
   5. Criar uma biblioetca de modelos e reports com Buscas:
      1. Analista
      2. R2 adj,
      3. Error medio ou acumulado na projecao
      4. Tamanaho da base
      5. Numero de var investigada ou utilizada na versao final
      6. Por commodity: soy, freight, wheat
      7. Por localidade (Mendely ou zotero pode ajudar)

5. *Simples* (Aqui que eu preciso tarbalhar mais na metodologia)

   1. Nosso negocio é muito dinamico e precisar de repostas rapidas
      (low inertia)
   2. Muito das nossas atividades nao necessitam de um modelo
      sofitiscado, o TIME é mais importante. Low hang fruits.
   3. Muitas areas sao under-staffs
   4. Actionable

6. *Tools (Software) 2 options*

   1. Powerfull (for modeler)
      1. Exploratory Analysis
      2. Easy to cumnicate with: Excel, Agview, SQL, Acces n R
   2. Super friendly (for modeler n analysts:Tableau)
      1. New analysys
      2. Complex projects
      3. Easy to cumnicate with: Excel, Agview, SQL, Acces n R

** Pragmatic programming principles

   1. DRY: Do not repeat yourself
   2. Write shy code (Keep your code decoupled)
      1. Law of least knowledgement.
      2. Decoupling n Law of Demeter
	 1. The Law of Demeter for functions states that any method of an
            obeject can call only methods belongs to:
	    1. itself
	    2. parameter that was passed in to the method
	    3. any object it created
	    4. any direct held component objects
   3. Design by Contratc
   4. Test Unit in mind
   5. Write code that writes code (Yasnippet)
   6. Refactor early n often
   7. Configure do not integrate
      1. read detail or parmeters form files
   8. crash early (good practice)

** Export
*** docx

    1. Change headers structure and create Dev Code n Analysis headers
    2. Set tags :noexport: to exclude subtree Dev Code n Analysis in the output
    3. org-html-export-as-html
    4. Save as html (Stop here to publish as html)
    5. Edit (delete) xml lines (first 3 lines)

       	#+BEGIN_SRC
       	<?xml version="1.0" encoding="utf-8"?>
       	<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
       	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
       	#+END_SRC

    6. Open it in MS word
    7. *Remember to turn on Navigation Panel in word:*
       1. View -> Tick Navigation Panel

*** html

    1. Change headers structure and create Dev Code n Analysis headers
    2. Set tags :noexport: to exclude subtree Dev Code n Analysis in the output
    3. org-html-export-as-html
    4. Save as html (Stop here to publish as html)
    5. Zip (folder do projeto)
       1. model_2014.org e/ou model_2014.docx
       2. model_2014.html
       3. figures

    Se zipar a arvore do projeto os links nao quebram inclusive para as
    planihas excel e para os dados usado.

*** mardown

    Eh mais popular do que orgmode

    1. org-md-export-to-markdown: C-c C-e m m

    Nao parece estar funcionando comletamente.  Principalmentes, links e
    tables. Code e headers estao ok

** Email Report results

   Escrever todos os pontos primeiro e depois mostrar resultado (/Aumentar a
   chance do kara ler os pontos antes de ir para os resultados/)

   Fazer copia do texto e criar planilha para prettfy tabelas, textos e
   graficos. Seu chefe pode querer rever e fazer alterações antes de vc
   enviar.

   Abordar os pontos:

   1. Dados
      1. Fontes do dados e data da ultima atualizacao
      2. Descrição breve dos dados e a taxa de amostragem: anual, mensal
         semanal usada

	 /Mensal: Colheita de soja.  SnD Cargill do dia 12/12/2014/

   2. Modelo (regression)
      1. R2 se nao for muito alto
      2. Termos sao significativos ou reportar algun pv um pouco maior
      3. Tamanho da amostra
	 1. Treino
	 2. Teste
      4. Periodo considerado
   3. key issues
      1. Algun fator imortante nao considerado
      2. Algun coeficiente que voc não eh muito confiante
      3. Dizer onde esta sendo conservador
   4. Resultado
      1. Expor dados com maior impacto no periodo da projecao considerada
         (explicar as maiores altas as maiores quedas, Picos)

	 Ex: Colheita de soja concentrada em Março e por tabela dos 3
         ultimos anos de Jan a Abril.

      2. Tabela com comparativo: mes anterior, ano passado opu outro periodo
         que julgar importante. Adicionar min e max e os respectivos
         comparativos

** Generates Rscripts

   1. C-c C-v t (org-tangle)

** Generates TAGS

   *ess-build-tags-for-directory*
   M-x ess-build-tags-for-directory run the shel script below for you
   Ask the directory to run rtags n then ask for file to save (TAGS)

   Unfortunately, these programs do not recognize R code syntax. They do
   allow tagging of arbitrary language files through regular expressions,
   but this is not sufficient for R.

   =================================
   R 2.9.0 onwards provides the rtags function as a tagging utility for R
   code. It parses R code files (using R's parser) and produces tags in
   Emacs' etags format.

   Steps:
   1. Build TAGS
      1. C-c '
      2. Menu ESS -> Process -> Start Process -> R
      3. run line by line code
   2. visit-tags-table (update hash)
   3. M-. visit tag (while point in function call)

    #+begin_src R :tangle ../../build_tags.R
      ## Generate TAGS file
      cat("Building TAGS file for the project ...\n")
      print(getwd()) ## project dir
      rtags(path="tools",recursive = TRUE,verbose=TRUE,ofile = "TAGS")
      rtags(path="models/soy/Rcode",recursive = TRUE,verbose=TRUE,
            append = TRUE,
            ofile = "TAGS")

      rtags(path="models/corn/Rcode",recursive = TRUE,verbose=TRUE,
            append = TRUE,
            ofile = "TAGS")

    #+end_src

** Build proj tree

   1. C-c C-c inside FSTREE
   2. Retirar arvore gerada fora bo bloco FSTREE
   3. Apagar alguns diretorios que vc nao precisa
   4. Os links paracem nao funcionar sem espaco depois deles. Entao adicione
      caso seja necessario

** Code blocks navigation n Run org-babel blocks inside emacs

 1. Colocar :session em todos os blocos para rodar tudo numa unica sessao do R
 2. Colocar :comments link para poder saltar do tangled file to respectivo org-babel-src
 3. Use: org-babel-switch-to-session n org-babel-pop-to-session para mudar
    para buffer do R
 4. C-c C-v g: org-babel-goto-named-src-block: Jump to org-babel block
 5. C-c C-j: Jump to orgmode header
 6. org-babel-tangle-jump-to-org in tamngled file to jump to org-babel-src
 7. org-babel-detangle propagate changes from tangled file to
    org-babel-block (But it is not working proper. At least for me and the
    way a try)

* Dev Code n Analysis						   :noexport:
