-*- Mode: org; mode: auto-fill; fill-column: 76 -*-

#+SEQ_TODO: TODO(t) STARTED(s!) SOMEDAY(S!) WAIT(w@/!) DELEGATE(e@/!) | DONE(d!/!)  CANCELED(c@)
#+STARTUP: overview
#+STARTUP: lognotestate
#+TAGS: noexport(n) export(e)
#+PROPERTY: Effort_ALL 0 0:10 0:20 0:30 1:00 2:00 4:00 6:00 8:00

#+TITLE:     Features_Selection
#+AUTHOR:    Leandro Fernandes
#+EMAIL:     leandro_h_fernandes@cargill.com
#+DATE:      <2015-12-09 Wed>

#+LANGUAGE:  en
#+TEXT:      GTD Agenda
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t TeX:t LaTeX:nil skip:t d:nil tags:not-in-toc
#+INFOJS_OPT: view:overview toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+LINK_UP:
#+LINK_HOME:
#+PROPERTY: Effort_ALL 0:05 0:15 0:30 0:45 1:00 1:30 2:00 3:00 4:00 5:00
#+TAGS: DATA(d) MODELLING(m) FORECASTING(f) WRITTING(w) REFACTORING(r)
#+COLUMNS: %40ITEM(Task) %TODO %17Effort(Estimated Effort){:} %CLOCKSUM %TAGS

# Local Variables:
# org-export-html-style: "   <style type=\"text/css\">
#    a:link, a:visited {font-style: italic; text-decoration: none; color: black; }
#    a:active {font-style: italic; texit-decoration: none; color: blue; } </style>
#   </style>"
# End:


#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export

* Introduction
  
  We started developing EDA (Exploratory Data analysis) library to help us
  data scientist project. We built routines to plot histograms, scatterplots
  and correlation matrix of target variables. Moreover we implemented a
  procedure to automatic select variables to speed up models development. We
  strong believe that the best EDA tool is the ability of the data scientist
  to make good questions about the data and also we know that these methods
  can end by adding garbage and noise to the analysis. But we think that if
  you use it with parsimony that might be useful. Moreover we compared some
  model selections methodology.

  * Stepwise
  * Relative importance

  Our learning objective: 
  * Start EDA library in R 
  * Compare variable investigation and selection methods 

  We also perform a small simulation. The idea is to use the developed EDA
  library to find out how close we can reach from the True Model (Simulated
  data) and how the noise and multicollinearity can disturb the end result

* Data

  One of the good things about data analysis is that you can test/check your
  ideas and learn with simulated data (Of course there is a limit for
  that). You can use simulated data to get more experience by analyzing
  data, double check if you understood a new theory or algorithm and also
  simulate scenario forecast.

  In this section, In order to test or EDA start-kit (eda library) we worked
  with simulated data. We choose linear relationship between dependent and
  independent variables because it is simple and appears very often in
  business analysis. We will work with non-linear relationship in the near
  future.

  Our data is summarized below:
  * 4 important variables 
  * 11 noise variable 
  * 2 correlated variable: x5 and x4 
  * 1 interaction relationship: x2:x3 
  * True model: 
    * $y = x1 + x1^2 + x2 + x2*x3 + x4 + \epsilon$ 
    * $\epsilon = N(0, \sigma^2)$ 
    * $\sigma^2$ is the Bayes error. 

  The code bellow loads all necessary libraries and the simulated data.

  #+begin_src R :tangle main.R :results output
    source('libs/features_selection.R')
    source('libs/data_simulator.R')

    data.size <- 128
    data.sim    <- DataSimulator(data.size)
    bayes.error = data.sim$GetSigma()
    true.model <- data.sim$GetTrueModelFormula()

    cat("Simulated data true model:\n")
    print(true.model)
    cat("Bayes error:\n")
    print(bayes.error)

    offset <- round(data.size*0.70)
    db       <- data.sim$GetData()
    train.db <- db[seq(1,offset,by=1),]
    test.db  <- db[seq(offset + 1,data.size,by=1),]
  #+end_src

  #+RESULTS:
  : Simulated data true model:
  : [1] "y ~ x1 + I(x1^2) + x2 + x2:x3 + x4"
  : Bayes error:
  : [1] 2

* Exploratory Data
  
  EDA is a critical early step in any data analysis where the goals are to
  get familiar with the data under analysis. In this phase you try discover
  typical values of variables, the type of each variable, distributions,
  missing values and outliers. Besides you start to investigate the
  relationship between predictor variables and the response variable.

  We implemented a basic approach that the author believes should be a start
  point in many analysis cases, but it is far away to cover all
  steps/techniques necessary in this phase. The libraries serve as a
  start-kit for EDA. In this step the ability of the data scientist to make
  good questions about the data is the best tool ever. But it is outside of
  the scope of this text.

  Using histograms, we can get an overview of the data, discover categorical
  and binary variables and also asses the variable distribution. The
  scatterplot also can give you a clue about importance of the variables
  that might be useful to explain the dependent variable.
  
  #+begin_src R :tangle main.R
    resp.var <- "y"
    eda <- DataExplorer(train.db, resp.var)

    eda$GetLinePainelDashBoard()
    eda$GetHistogramDashBoard()
    eda$GetScatterPainelDashBoard()
    eda$GetAutocorrelationDashBoard()

  #+end_src

  The line plot bellow can help us check with variables have any trend or
  seasonality (stationary issues).
  
  [[file:figures/eda_line_plot.png]]

  The histograms of variables are helpful to discover binary variables,
  categorical variables and also are useful to get an overview of the
  variable distribution.
  
  [[file:figures/eda_histograms.png]]

  The scatter plot helps us to get first insight of the variables
  relationships. For instance it is easy to see that response variable is
  correlated with predictor x1.

  [[file:figures/eda_scatterplot.png]]

  Moreover the autocorrelation plot is important because can suggest the use
  of autoregressive models.
    
  [[file:figures/eda_autocorr.png]]

  The correlation matrix is also complementary to scatter plot and can help
  you to select model variables and assess multicollinearity.

  #+begin_src R :tangle main.R
    eda$GetCorrDashBoard()
  #+end_src
 
  [[file:figures/eda_matrix_correlation.png]]
 
* Features Selection

  In the code bellow we use stepwise exhaustive model selection to
  semi-automatic choose models and model's variable. The method is described
  in R help function (Package: regsubsets). We use these routines to build a
  plot of both rmse and adjusted r-squared vs model complexity (number of
  parameters for regressions). The dashed red line is the Bayes error. These
  graphs can give you an overview of the bias variance trade-off.
  
  #+begin_src R :tangle main.R
    data.formula <- data.sim$GetVarsFormula()
    reg.formula <- formula(paste0(data.formula," + I(x1^2) + x2:x3"))

    cat("Investigated relation:\n")
    print(reg.formula)

    nvmax <- 15
    reg.exp <- RegsubsetExplorer(train.db,test.db,reg.formula,nvmax,
                                 nbest=1,really.big=FALSE,force.in=NULL)

    reg.exp$GetRegsubsetDashBoard(bayes.error)
  #+end_src

  [[file:figures/reg_subset_adjr2.png]]

  [[file:figures/reg_subset_rmse.png]]

  We repeated the experiment above, but now we use xgboost to build the same
  graphs. But now model complexity is related with the number of tree in the
  GBM algorithm.

  #+begin_src R :tangle main.R
    param <- list("objective" = "reg:linear",
                  "eta" = 0.1,
                  "subsample" = 0.80,
                  "colsample_bytree" = 0.80,
                  "scale_pos_weight" = 1.00,
                  "silent" = 1,
                  "max_depth" = 7,
                  "seed" = 19)

    number.of.models <- 15
    xgb.exp <- XGBoostExplorer(train.db, test.db, "y", number.of.models,
                               param)

    xgb.exp$GetXGBoostDashBoard(bayes.error)

 #+end_src
 
  [[file:figures/xgb_pseudo_squared.png]]

  [[file:figures/xgb_rmse.png]]
  
  Analyzing the graphs we should conclude that in the regression case, the
  best model might be the number 4.
  
  #+begin_src R :tangle main.R
    m4 <-reg.exp$GetModelRegSubset(4,TRUE)
    summary(m4)
  #+end_src

  #+BEGIN_EXAMPLE
  Print Model:  4 
  | names       | coefs   |
  |-------------+---------|
  | (Intercept) | 5.8824  |
  | x1          | 2.3765  |
  | x2          | 1.5576  |
  | I(x1^2)     | -0.0875 |
  | x2:x3       | 0.4630  |
  Print Model:  4  neighbors
  | n      | adjr2  |
  |--------+--------|
  | 3.0000 | 0.8109 |
  | 4.0000 | 0.8385 |
  | 5.0000 | 0.8468 |
  Printing model formula
  [1] "y  ~  x1 + x2 + I(x1^2) + x2:x3"
  ---------------------------------
  Call:
  lm(formula = model.formula, data = train.db)

  Residuals:
  Min      1Q  Median      3Q     Max 
  -3.4318 -1.1863 -0.0583  1.2090  4.1711 

  Coefficients:
  Estimate Std. Error t value Pr(>|t|)    
  (Intercept)  5.88239    0.87694   6.708 2.07e-09 ***
  x1           2.37646    0.25813   9.207 2.06e-14 ***
  x2           1.55759    0.20930   7.442 7.41e-11 ***
  I(x1^2)     -0.08750    0.02211  -3.958 0.000156 ***
  x2:x3        0.46302    0.04720   9.811 1.24e-15 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 1.794 on 85 degrees of freedom
  Multiple R-squared:  0.8457,	Adjusted R-squared:  0.8385 
  F-statistic: 116.5 on 4 and 85 DF,  p-value: < 2.2e-16
  #+END_EXAMPLE  
  
  The model is quite similar to the True Model. Only variable x4 is
  missing. Moreover the rmse in test is close to Bayes error, however the
  train error is below the Bayes error.

  Repeating the analysis but now using xgboost. The best model would be also
  number 4 (Model with 20 trees). Note that the rmse is almost twice the
  Bayes error and rmse in train data set is equal to Bayes error. You can
  plot relative importance variable with the code bellow:

  #+begin_src R :tangle main.R
    xgb.exp$PlotRelativeImportance()
  #+end_src

  [[file:figures/xgb_rel_importance.png]]

  Based on the graph, this technique is missing interactions variables
  (x2:x3) and also added a noise variable as an important variable (x8).
  
* Conclusion

  We started developing EDA library in R to help us investigate relationship
  among variables in a data set. We use visualization techniques and also 2
  different models selection approach. Those techniques have to be used with
  parsimony, but can speed up your analysis.


