-*- Mode: org; mode: auto-fill; fill-column: 76 -*-

#+SEQ_TODO: TODO(t) STARTED(s!) SOMEDAY(S!) WAIT(w@/!) DELEGATE(e@/!) | DONE(d!/!)  CANCELED(c@)
#+STARTUP: overview
#+STARTUP: lognotestate
#+TAGS: noexport(n) export(e)
#+PROPERTY: Effort_ALL 0 0:10 0:20 0:30 1:00 2:00 4:00 6:00 8:00

#+TITLE:     Features_Selection
#+AUTHOR:    Leandro Fernandes
#+EMAIL:     leandro_h_fernandes@cargill.com
#+DATE:      <2015-12-09 Wed>

#+LANGUAGE:  en
#+TEXT:      GTD Agenda
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t TeX:t LaTeX:nil skip:t d:nil tags:not-in-toc
#+INFOJS_OPT: view:overview toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+LINK_UP:
#+LINK_HOME:
#+PROPERTY: Effort_ALL 0:05 0:15 0:30 0:45 1:00 1:30 2:00 3:00 4:00 5:00
#+TAGS: DATA(d) MODELLING(m) FORECASTING(f) WRITTING(w) REFACTORING(r)
#+COLUMNS: %40ITEM(Task) %TODO %17Effort(Estimated Effort){:} %CLOCKSUM %TAGS

# Local Variables:
# org-export-html-style: "   <style type=\"text/css\">
#    a:link, a:visited {font-style: italic; text-decoration: none; color: black; }
#    a:active {font-style: italic; texit-decoration: none; color: blue; } </style>
#   </style>"
# End:


#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export

* Introduction
  
  We started developing EDA (Exploratory Data analysis) library to help us
  data scientist project. We built routines to plot histograms, scatterplots
  and correlation matrix of target variables. Moreover we implemented a
  procedure to automatic select variables to speed up models development. We
  strong believe that the best EDA tool is the ability of the data scientist
  to make good questions about the data and also we know that these methods
  can end by adding garbage and noise to the analysis. But we think that if
  you use it with parsimony that might be useful. Moreover we compared some
  model selections methodology.

  * Stepwise
  * Relative importance

  Our learning objective: 
  * Start EDA library in R 
  * Compare variable investigation and selection methods 

  We also perform a small simulation. The idea is to use the developed EDA
  library to find out how close we can reach from the True Model (Simulated
  data) and how the noise and multicollinearity can disturb the end result

* Data

  One of the good things about data analysis is that you can test/check your
  ideas and learn with simulated data (Of course there is a limit for
  that). You can use simulated data to get more experience by analyzing
  data, double check if you understood a new theory or algorithm and also
  simulate scenario forecast.

  In this section, In order to test or EDA start-kit (eda library) we worked
  with simulated data. We choose linear relationship between dependent and
  independent variables because it is simple and appears very often in
  business analysis. We will work with non-linear relationship in the near
  future.

  Our data is summarized below:
  * 4 important variables 
  * 11 noise variable 
  * 2 correlated variable: x5 and x4 
  * 1 interaction relationship: x2:x3 
  * True model: 
    * $y = x1 + x1^2 + x2 + x2*x3 + x4 + \epsilon$ 
    * $\epsilon = N(0, \sigma^2)$ 
    * $\sigma^2$ is the Bayes error. 

  The code bellow loads all necessary libraries and the simulated data.

  #+begin_src R :tangle main.R :results output
    source('libs/features_selection.R')
    source('libs/data_simulator.R')

    data.size <- 128
    data.sim    <- DataSimulator(data.size)
    bayes.error = data.sim$GetSigma()
    true.model <- data.sim$GetTrueModelFormula()

    cat("Simulated data true model:\n")
    print(true.model)
    cat("Bayes error:\n")
    print(bayes.error)

    offset <- round(data.size*0.70)
    db       <- data.sim$GetData()
    train.db <- db[seq(1,offset,by=1),]
    test.db  <- db[seq(offset + 1,data.size,by=1),]
  #+end_src

  #+RESULTS:
  : Simulated data true model:
  : [1] "y ~ x1 + I(x1^2) + x2 + x2:x3 + x4"
  : Bayes error:
  : [1] 2

* Exploratory Data
  
  EDA is a critical early step in any data analysis where the goals are to
  get familiar with the data under analysis. In this phase you try discover
  typical values of variables, the type of each variable, distributions,
  missing values and outliers. Besides you start to investigate the
  relationship between predictor variables and the response variable.

  We implemented a basic approach that the author believes should be a start
  point in many analysis cases, but it is far away to cover all
  steps/techniques necessary in this phase. The libraries serve as a
  start-kit for EDA. In this step the ability of the data scientist to make
  good questions about the data is the best tool ever. But it is outside of
  the scope of this text.

  Using histograms, we can get an overview of the data, discover categorical
  and binary variables and also asses the variable distribution. The
  scatterplot also can give you a clue about importance of the variables
  that might be useful to explain the dependent variable.
  
  #+begin_src R :tangle main.R
    eda <- DataExplorer(train.db, "y")

    eda$GetLinePainelDashBoard()
    eda$GetHistogramDashBoard()
    eda$GetScatterPainelDashBoard()
    eda$GetAutocorrelationDashBoard()

  #+end_src

  The line plot bellow can help us check with variables have any trend or
  seasonality (stationary issues).
  
  [[file:figures/eda_line_plot.png]]

  The histograms of variables are helpful to discover binary variables,
  categorical variables and also are useful to get an overview of the
  variable distribution.
  
  [[file:figures/eda_histograms.png]]

  The scatter plot helps us to get first insight of the variables
  relationships. For instance it is easy to see that response variable is
  correlated with predictor x1.

  [[file:figures/eda_scatterplot.png]]

  Moreover the autocorrelation plot is important because can suggest the use
  of autoregressive models.
    
  [[file:figures/eda_autocorr.png]]

  The correlation matrix is also complementary to scatter plot and can help
  you to select model variables and assess multicollinearity.

  #+begin_src R :tangle main.R
    eda$GetCorrDashBoard()
  #+end_src
 
  [[file:figures/eda_matrix_correlation.png]]
 
* Features Selection

  In the code bellow we use stepwise exhaustive model selection to
  semi-automatic choose models and model's variable. The method is described
  in R help function (Package: regsubsets). We use these routines to build a
  plot of both rmse and adjusted r-squared vs model complexity (number of
  parameters for regressions). The dashed red line is the Bayes error. These
  graphs can give you an overview of the bias variance trade-off.
  
  #+begin_src R :tangle main.R
    data.formula <- data.sim$GetVarsFormula()
    reg.formula <- formula(paste0(data.formula," + I(x1^2) + x2:x3"))

    cat("Investigated relation:\n")
    print(reg.formula)

    nvmax <- 15
    reg.exp <- RegsubsetExplorer(train.db,test.db,reg.formula,nvmax,
                                 nbest=1,really.big=FALSE,force.in=NULL)

    reg.exp$GetRegsubsetDashBoard(bayes.error)
  #+end_src

  [[file:figures/reg_subset_adjr2.png]]

  [[file:figures/reg_subset_rmse.png]]

  We repeated the experiment above, but now we use xgboost to build the same
  graphs. But now model complexity is related with the number of tree in the
  GBM algorithm.

  #+begin_src R :tangle main.R
    param <- list("objective" = "reg:linear",
                  "eta" = 0.1,
                  "subsample" = 0.80,
                  "colsample_bytree" = 0.80,
                  "scale_pos_weight" = 1.00,
                  "silent" = 1,
                  "max_depth" = 7,
                  "seed" = 19)

    number.of.models <- 15
    xgb.exp <- XGBoostExplorer(train.db, test.db, "y", number.of.models,
                               param)

    xgb.exp$GetXGBoostDashBoard(bayes.error)

 #+end_src
 
  [[file:figures/xgb_pseudo_squared.png]]

  [[file:figures/xgb_rmse.png]]
  
  Analyzing the graphs we should conclude that in the regression case, the
  best model might be the number 4.
  
  #+begin_src R :tangle main.R
    m4 <-reg.exp$GetModelRegSubset(4,TRUE)
    summary(m4)
  #+end_src

  #+BEGIN_EXAMPLE
  Print Model:  4 
  | names       | coefs   |
  |-------------+---------|
  | (Intercept) | 5.8824  |
  | x1          | 2.3765  |
  | x2          | 1.5576  |
  | I(x1^2)     | -0.0875 |
  | x2:x3       | 0.4630  |
  Print Model:  4  neighbors
  | n      | adjr2  |
  |--------+--------|
  | 3.0000 | 0.8109 |
  | 4.0000 | 0.8385 |
  | 5.0000 | 0.8468 |
  Printing model formula
  [1] "y  ~  x1 + x2 + I(x1^2) + x2:x3"
  ---------------------------------
  Call:
  lm(formula = model.formula, data = train.db)

  Residuals:
  Min      1Q  Median      3Q     Max 
  -3.4318 -1.1863 -0.0583  1.2090  4.1711 

  Coefficients:
  Estimate Std. Error t value Pr(>|t|)    
  (Intercept)  5.88239    0.87694   6.708 2.07e-09 ***
  x1           2.37646    0.25813   9.207 2.06e-14 ***
  x2           1.55759    0.20930   7.442 7.41e-11 ***
  I(x1^2)     -0.08750    0.02211  -3.958 0.000156 ***
  x2:x3        0.46302    0.04720   9.811 1.24e-15 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 1.794 on 85 degrees of freedom
  Multiple R-squared:  0.8457,	Adjusted R-squared:  0.8385 
  F-statistic: 116.5 on 4 and 85 DF,  p-value: < 2.2e-16
  #+END_EXAMPLE  
  
  The model is quite similar to the True Model. Only variable x4 is
  missing. Moreover the rmse in test is close to Bayes error, however the
  train error is below the Bayes error.

  Repeating the analysis but now using xgboost. The best model would be also
  number 4 (Model with 20 trees). Note that the rmse is almost twice the
  Bayes error and rmse in train data set is equal to Bayes error. You can
  plot relative importance variable with the code bellow:

  #+begin_src R :tangle main.R
    xgb.exp$PlotRelativeImportance()
  #+end_src

  [[file:figures/xgb_rel_importance.png]]

  Based on the graph, this technique is missing interactions variables
  (x2:x3) and also added a noise variable as an important variable (x8).
  
* Conclusion

  We started developing EDA library in R to help us investigate relationship
  among variables in a data set. We use visualization techniques and also 2
  different models selection approach. Those techniques have to be used with
  parsimony, but can speed up your analysis.

* Appendix
** Project Tree (Folders)
   #+BEGIN_FSTREE: . :relative-links t :non-recursive nil
   #+END_FSTREE
* Automate system 						   :noexport:
** Loaded Questions

   *Projetos precisam ter comeco , muio e fim alem de um objetivo claro.*

   1. Goals scope:
      1. Generic one
	 1. Qual eh o escopo? O objetivo? Nunca se esqueca disto.
	    Construir uma ferramenta para calssifcar se um email eh spam ou NAO
      2. Specific

	 Usando os dados do site S, investigar as vars Xs e construir um
         calssificador de emails (SPAM ou NAO) utilizando uma das tecnicas:
         T1, T2 or T3.

   2. Data scope: *MECE* (mutually exclude collected exhaustive)
      1. Data
	 1. Quais dados tenho confianca? E quais nao tenho tanta assim?
	 2. Os dados sao adequados para o escopo do modelo?
	 3. Tenho projecao destes dado? Sao boas estas projecoes?
      2. Ys:
	 1. Quais periodos tem maior volatilidade?
	 2. Quais periodos podemos ter inversao (As veze sobe as vezes cai)?
            Alerta onde podemos erra a direcao. (Preciso calcular as variacoes
            temporais)
      3. Xs:
	 1. Definir quais variaveis serao investigadas. Manter o FOCO
	 2. ADD alguma coisa aqui
   3. Modelo
      1. Oq nao considerei qual seria o palpite intuitivo de como ele
         afetaria minha projecao? Consigo ver esas relacoes olhando para os
         residuos e estes fatores que nao estou considerando?
   4. Res:
      1. Quais periodos os residuos apresentam bias?
      2. Qual periodo os residuos apresentam grande variacao? Posso errar pr
         pouco ou por muito.
   5. Forecast
      1. Como estao as projecoes de Xs em relacao a base historica?
      2. Como minha projecao estah em relacao a base historica? (Acima do
         ano passad abaixo. Faz sentido?)
      3. As variacoes temporais (mensai, anuais) da projecao sao compativeis
         com estas mesamas variacoes na base? Faz sebtido?
   6. Aplicacao do modelo (Impacto) *<=* (Um dos mais importnates dos items)
      1. Quais perguntas eu consigo responder com o atual modelo?
	 1. Pensar na aplicaco ao negocio
      2. Tipos de perguntas comuns para responder
	 1. Oq vai acontercer se ocorrer uma reducao de 10%X na var Y?
	 2. Pq aconteceu esta queda.
	 3. Oq irá acontecer?

** Analytical process Concepts

1. *Versionado* (SVN, GIT e tortoise)

   1. *Evolui continuamente a passos pequenos*
   2. Evita re-trabalho
   3. Registro do projeto no tempo. Mantém analise transparente.

2. *Work in pairs*

   1. Ajuda prevenir blind-spots.
   2. Acelera curvas de aprendizado
   3. *Permite construcao de buy-in qdo ooutro par eh da area cliente*

3. *Reproduzivel* Porque?

   1. Nos mantém honesto,
   2. Permite rever os passos,
   3. Permite outros rodarem o modelo e assim permite aprendizado
   4. As coisas continuam funcionado caso eu nao esteja

4. *Documentacao Interna* (Confidencial e pertence ao GTABR)

   1. Salvar a expertise adquirida.
   2. Ajuda organizar suas ideias. (Qdo vc se obriga a escrever isto de
      forac a pensar e rever suas ideais)
   3. Qdo for questionado por algo que fez muito tempo atrás, pode-se
      consultar a doc.
   4. Permite outros aproveitar a experiencia adquirida e/ou adaptar
      para o seu caso.
   5. Criar uma biblioetca de modelos e reports com Buscas:
      1. Analista
      2. R2 adj,
      3. Error medio ou acumulado na projecao
      4. Tamanaho da base
      5. Numero de var investigada ou utilizada na versao final
      6. Por commodity: soy, freight, wheat
      7. Por localidade (Mendely ou zotero pode ajudar)

5. *Simples* (Aqui que eu preciso tarbalhar mais na metodologia)

   1. Nosso negocio é muito dinamico e precisar de repostas rapidas
      (low inertia)
   2. Muito das nossas atividades nao necessitam de um modelo
      sofitiscado, o TIME é mais importante. Low hang fruits.
   3. Muitas areas sao under-staffs
   4. Actionable

6. *Tools (Software) 2 options*

   1. Powerfull (for modeler)
      1. Exploratory Analysis
      2. Easy to cumnicate with: Excel, Agview, SQL, Acces n R
   2. Super friendly (for modeler n analysts:Tableau)
      1. New analysys
      2. Complex projects
      3. Easy to cumnicate with: Excel, Agview, SQL, Acces n R

** Pragmatic programming principles

   1. DRY: Do not repeat yourself
   2. Write shy code (Keep your code decoupled)
      1. Law of least knowledgement.
      2. Decoupling n Law of Demeter
	 1. The Law of Demeter for functions states that any method of an
            obeject can call only methods belongs to:
	    1. itself
	    2. parameter that was passed in to the method
	    3. any object it created
	    4. any direct held component objects
   3. Design by Contratc
   4. Test Unit in mind
   5. Write code that writes code (Yasnippet)
   6. Refactor early n often
   7. Configure do not integrate
      1. read detail or parmeters form files
   8. crash early (good practice)

** Export
*** docx

    1. Change headers structure and create Dev Code n Analysis headers
    2. Set tags :noexport: to exclude subtree Dev Code n Analysis in the output
    3. org-html-export-as-html
    4. Save as html (Stop here to publish as html)
    5. Edit (delete) xml lines (first 3 lines)

       	#+BEGIN_SRC
       	<?xml version="1.0" encoding="utf-8"?>
       	<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
       	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
       	#+END_SRC

    6. Open it in MS word
    7. *Remember to turn on Navigation Panel in word:*
       1. View -> Tick Navigation Panel

*** html

    1. Change headers structure and create Dev Code n Analysis headers
    2. Set tags :noexport: to exclude subtree Dev Code n Analysis in the output
    3. org-html-export-as-html
    4. Save as html (Stop here to publish as html)
    5. Zip (folder do projeto)
       1. model_2014.org e/ou model_2014.docx
       2. model_2014.html
       3. figures

    Se zipar a arvore do projeto os links nao quebram inclusive para as
    planihas excel e para os dados usado.

*** mardown

    Eh mais popular do que orgmode

    1. org-md-export-to-markdown: C-c C-e m m

    Nao parece estar funcionando comletamente.  Principalmentes, links e
    tables. Code e headers estao ok

** Email Report results

   Escrever todos os pontos primeiro e depois mostrar resultado (/Aumentar a
   chance do kara ler os pontos antes de ir para os resultados/)

   Fazer copia do texto e criar planilha para prettfy tabelas, textos e
   graficos. Seu chefe pode querer rever e fazer alterações antes de vc
   enviar.

   Abordar os pontos:

   1. Dados
      1. Fontes do dados e data da ultima atualizacao
      2. Descrição breve dos dados e a taxa de amostragem: anual, mensal
         semanal usada

	 /Mensal: Colheita de soja.  SnD Cargill do dia 12/12/2014/

   2. Modelo (regression)
      1. R2 se nao for muito alto
      2. Termos sao significativos ou reportar algun pv um pouco maior
      3. Tamanho da amostra
	 1. Treino
	 2. Teste
      4. Periodo considerado
   3. key issues
      1. Algun fator imortante nao considerado
      2. Algun coeficiente que voc não eh muito confiante
      3. Dizer onde esta sendo conservador
   4. Resultado
      1. Expor dados com maior impacto no periodo da projecao considerada
         (explicar as maiores altas as maiores quedas, Picos)

	 Ex: Colheita de soja concentrada em Março e por tabela dos 3
         ultimos anos de Jan a Abril.

      2. Tabela com comparativo: mes anterior, ano passado opu outro periodo
         que julgar importante. Adicionar min e max e os respectivos
         comparativos

** Generates Rscripts

   1. C-c C-v t (org-tangle)

** Generates TAGS

   *ess-build-tags-for-directory*
   M-x ess-build-tags-for-directory run the shel script below for you
   Ask the directory to run rtags n then ask for file to save (TAGS)

   Unfortunately, these programs do not recognize R code syntax. They do
   allow tagging of arbitrary language files through regular expressions,
   but this is not sufficient for R.

   =================================
   R 2.9.0 onwards provides the rtags function as a tagging utility for R
   code. It parses R code files (using R's parser) and produces tags in
   Emacs' etags format.

   Steps:
   1. Build TAGS
      1. C-c '
      2. Menu ESS -> Process -> Start Process -> R
      3. run line by line code
   2. visit-tags-table (update hash)
   3. M-. visit tag (while point in function call)

    #+begin_src R :tangle ../../build_tags.R
      ## Generate TAGS file
      cat("Building TAGS file for the project ...\n")
      print(getwd()) ## project dir
      rtags(path="tools",recursive = TRUE,verbose=TRUE,ofile = "TAGS")
      rtags(path="models/soy/Rcode",recursive = TRUE,verbose=TRUE,
            append = TRUE,
            ofile = "TAGS")

      rtags(path="models/corn/Rcode",recursive = TRUE,verbose=TRUE,
            append = TRUE,
            ofile = "TAGS")

    #+end_src

** Build proj tree

   1. C-c C-c inside FSTREE
   2. Retirar arvore gerada fora bo bloco FSTREE
   3. Apagar alguns diretorios que vc nao precisa
   4. Os links paracem nao funcionar sem espaco depois deles. Entao adicione
      caso seja necessario

** Code blocks navigation n Run org-babel blocks inside emacs

 1. Colocar :session em todos os blocos para rodar tudo numa unica sessao do R
 2. Colocar :comments link para poder saltar do tangled file to respectivo org-babel-src
 3. Use: org-babel-switch-to-session n org-babel-pop-to-session para mudar
    para buffer do R
 4. C-c C-v g: org-babel-goto-named-src-block: Jump to org-babel block
 5. C-c C-j: Jump to orgmode header
 6. org-babel-tangle-jump-to-org in tamngled file to jump to org-babel-src
 7. org-babel-detangle propagate changes from tangled file to
    org-babel-block (But it is not working proper. At least for me and the
    way a try)


